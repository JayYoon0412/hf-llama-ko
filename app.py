import argparse
import os
from time import sleep
from typing import List
from threading import Thread

import gradio as gr
from transformers import AutoTokenizer

class LLMChatHandler():
    def __init__(self, model_id: str, use_vllm: bool = False):
        self.use_vllm = use_vllm
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("")
        ]
        self.initial_instruction = (
            """
You are a chatbot designed to help users determine which hospital department they should visit based on their symptoms. Engage the user in a conversation to gather detailed information about their condition. Here are the steps to follow:

1. **Greeting and Introduction**: Start by greeting the user and introducing yourself as a chatbot here to assist them in finding the right hospital department.

2. **Symptom Inquiry**: Ask the user to describe their symptoms in detail. Encourage them to include the following information:
   - Location of pain or discomfort
   - Severity and duration of the symptoms
   - Any accompanying symptoms (e.g., fever, dizziness, nausea)
   - Any pre-existing conditions or recent events that might be related

3. **Specific Questions**: Use specific questions to get more precise information:
   - Are you experiencing chest pain? If so, for how long and how intense is it?
   - Do you have any respiratory issues, like shortness of breath or persistent coughing?
   - Are there any gastrointestinal symptoms, such as stomach pain, nausea, or vomiting?
   - Have you had any recent injuries or accidents?

4. **Additional Information**: Ask if there are any other symptoms or details they think might be relevant.

5. **Confirmation and Department Suggestion**: Based on the information provided, confirm the symptoms and suggest the appropriate hospital department.

Example conversation:
User: "I have a severe headache and nausea."
Chatbot: "I'm sorry to hear that. How long have you been experiencing these symptoms? Are there any other symptoms, such as sensitivity to light or fever?"

Remember to be empathetic and reassuring throughout the conversation. Your goal is to gather enough information to accurately direct the user to the correct hospital department.
When the user inputs the keyword "Recommend", Based on the information provided, confirm the symptoms and suggest the appropriate hospital department immediately.
ë„ˆëŠ” ì¦ìƒì— ì•Œë§žì€ ë³‘ì›ì„ ì¶”ì²œí•´ì£¼ëŠ” ì±—ë´‡ì´ì•¼
ì‚¬ìš©ìžëŠ” ì•„í”Œ ë•Œ ë³‘ì›ì˜ ì„¸ë¶€ ë¶„ê³¼, ì§„ë£Œê³¼ ì¤‘ ì–´ë””ë¥¼ ì„ íƒí•´ì•¼í• ì§€ ì•Œê³ ì‹¶ì€ë° ë„¤ê°€ ì ì ˆí•œ ë³‘ì›ì„ ì¶”ì²œí•´ì¤˜.
í˜„ìž¬ ì¡´ìž¬í•˜ëŠ” ì§„ë£Œê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
ë‚´ê³¼ì—ëŠ” í˜¸í¡ê¸°ë‚´ê³¼, ìˆœí™˜ê¸°ë‚´ê³¼, ì†Œí™”ê¸°ë‚´ê³¼, í˜ˆì•¡ì¢…ì–‘ë‚´ê³¼, ë‚´ë¶„ë¹„ëŒ€ì‚¬ë‚´ê³¼, ì•Œë ˆë¥´ê¸°ë‚´ê³¼, ì‹ ìž¥ë‚´ê³¼, ê°ì—¼ë‚´ê³¼, ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼, ë‚´ê³¼(ì¼ë°˜), ë‚´ê³¼(ìž…ì›ì˜í•™)ë¡œ ì´ 11ê°œì˜ ì§„ë£Œê³¼ê°€ ì¡´ìž¬í•œë‹¤.ì™¸ê³¼ì—ëŠ” ê°„ë‹´ì·Œì™¸ê³¼, ëŒ€ìž¥í•­ë¬¸ì™¸ê³¼, ì´ì‹í˜ˆê´€ì™¸ê³¼, ìœ„ìž¥ê´€ì™¸ê³¼, ìœ ë°©ë‚´ë¶„ë¹„ì™¸ê³¼, ì™¸ìƒì™¸ê³¼, ì™¸ê³¼(ì¼ë°˜), ì™¸ê³¼(ìž…ì›ì˜í•™)ë¡œ ì´ 8ê°œì˜ ì§„ë£Œê³¼ê°€ ì¡´ìž¬í•œë‹¤.ê·¸ë¦¬ê³  ë‚´ê³¼ì™€ ì™¸ê³¼ ì´ì™¸ì˜ ê³¼ì˜ ê²½ìš° ì‚°ë¶€ì¸ê³¼, ë§ˆì·¨í†µì¦ì˜í•™ê³¼, ë¹„ë‡¨ì˜í•™ê³¼(ë¹„ë‡¨ê¸°ê³¼), ì‹ ê²½ì™¸ê³¼, ì‘ê¸‰ì˜í•™ê³¼, ìž„ìƒì•½ë¦¬í•™ê³¼, ê°€ì •ì˜í•™ê³¼, ë°©ì‚¬ì„ ì¢…ì–‘í•™ê³¼, ì„±í˜•ì™¸ê³¼, ì•ˆê³¼, ì˜ê³µí•™ê³¼, ìž„ìƒìœ ì „ì²´ì˜í•™ê³¼, ë³‘ë¦¬ê³¼, ì‹ ê²½ê³¼, ì˜ìƒì˜í•™ê³¼, ì´ë¹„ì¸í›„ê³¼, ìž¬í™œì˜í•™ê³¼, ì •ì‹ ê³¼í•™ì˜í•™ê³¼, ì§„ë‹¨ê²€ì‚¬ì˜í•™ê³¼, í‰ë¶€ì™¸ê³¼, ì •í˜•ì™¸ê³¼, í”¼ë¶€ê³¼, ì¤‘í™˜ìžì˜í•™ê³¼, í•µì˜í•™ê³¼ ë“±ì´ ì¡´ìž¬í•œë‹¤.
ì‚¬ìš©ìžëŠ” ìžê¸° ì¦ìƒì„ ë§í•˜ê³ , ë‚˜ì´ì™€ ì„±ë³„ì„ ì•Œë ¤ì¤„ ê±°ì•¼. ì •ë³´ë¥¼ ê³ ë ¤í•´ì„œ ì¶”ì²œí•˜ëŠ” ì§„ë£Œê³¼ë¥¼ ì•Œë ¤ì¤˜. 
ì‚¬ìš©ìžì˜ í˜„ìž¬ ìœ„ì¹˜ë¥¼ í† ëŒ€ë¡œ ì ì ˆí•œ ë³‘ì›ì˜ ì´ë¦„ì„ ì•Œë ¤ì¤˜ë„ ì¢‹ì•„.
ì‚¬ìš©ìžê°€ "ì§„ë£Œê³¼ ì¶”ì²œ ë°”ëžŒ"ì´ë¼ê³  ìž‘ì„±í•˜ë©´ ë”ì´ìƒì˜ ê¼¬ë¦¬ ì§ˆë¬¸ ì—†ì´ ë°”ë¡œ ì§„ë£Œê³¼ë¥¼ ë¬´ì¡°ê±´ ì¶”ì²œí•´ì¤˜.
"""
        )
        if use_vllm:
            from vllm.engine.arg_utils import AsyncEngineArgs
            from vllm.engine.async_llm_engine import AsyncLLMEngine
            engine_args = AsyncEngineArgs(
                model=model_id,
                tokenizer=None,
                tokenizer_mode="auto",
                trust_remote_code=True,
                quantization="awq" if "awq" in model_id or "AWQ" in model_id else None,
                dtype="auto",
            )
            self.vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)
        else:
            from transformers import AutoModelForCausalLM
            self.hf_model = AutoModelForCausalLM.from_pretrained(
                model_id,
                trust_remote_code=True,
                attn_implementation="flash_attention_2",
                torch_dtype="auto",
                device_map="auto")

    def chat_history_to_prompt(self, message: str, history: List[List[str]]) -> str:
        conversation = [{"role": "assistant", "content": self.initial_instruction}]
        for h in history:
            user_text, assistant_text = h
            conversation += [
                {"role": "user", "content": user_text},
                {"role": "assistant", "content": assistant_text},
            ]

        conversation.append({"role": "user", "content": message})
        prompt = self.tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
        return prompt

    async def chat_function(self, message, history):
        prompt = self.chat_history_to_prompt(message, history)
        if self.use_vllm:
            response_generator = self.chat_function_vllm(prompt)
            async for text in response_generator:
                text = self.filter_text(text)  # Filter the text
                yield text
        else:
            response_generator = self.chat_function_hf(prompt)
            for text in response_generator:
                text = self.filter_text(text)  # Filter the text
                yield text

    async def chat_function_vllm(self, prompt):
        from vllm import SamplingParams
        from vllm.utils import random_uuid
        sampling_params = SamplingParams(
            stop_token_ids=self.terminators,
            max_tokens=2048,
            temperature=0.6,
            top_p=0.9,
            repetition_penalty=1.2)
        results_generator = self.vllm_engine.generate(prompt, sampling_params, random_uuid())
        async for request_output in results_generator:
            response_txt = ""
            for output in request_output.outputs:
                if output.text not in self.terminators:
                    response_txt += output.text
            yield response_txt

    def chat_function_hf(self, prompt):
        from transformers import pipeline, TextIteratorStreamer
        streamer = TextIteratorStreamer(self.tokenizer)
        pipe = pipeline(
            "text-generation",
            model=self.hf_model,
            tokenizer=self.tokenizer,
            eos_token_id=self.terminators,
            max_length=2048,  # Control maximum tokens here
            temperature=0.6,
            top_p=0.9,
            repetition_penalty=1.2,
            return_full_text=False,
            streamer=streamer
        )
        t = Thread(target=pipe, args=(prompt,))
        t.start()

        generated_text = ""
        for new_text in streamer:
            generated_text += new_text
            yield generated_text

        t.join()

    def filter_text(self, text: str) -> str:
        # Post-processing step to remove unwanted emojis and special characters
        import re
        text = re.sub(r'[^\w\s,.?!-]', '', text)  # Allow only alphanumeric characters and some punctuation
        return text

def close_app():
    gr.Info("Terminated the app!")
    sleep(1)
    os._exit(0)

def main(args):
    print(f"Loading the model {args.model_id}...")
    hdlr = LLMChatHandler(args.model_id, args.use_vllm)

    with gr.Blocks(title=f"ðŸ¤— Chatbot with {args.model_id}", fill_height=True) as demo:
        with gr.Row():
            gr.Markdown(
                f"<h2>Chatbot with ðŸ¤— {args.model_id} ðŸ¤—</h2>"
                "<h3>Interact with LLM using chat interface!<br></h3>"
                f"<h3>Original model: <a href='https://huggingface.co/{args.model_id}' target='_blank'>{args.model_id}</a></h3>")
        gr.ChatInterface(hdlr.chat_function)
        with gr.Row():
            close_button = gr.Button("Close the app", variant="stop")
            close_button.click(
                fn=lambda: gr.update(interactive=False), outputs=[close_button]
            ).then(fn=close_app)

    demo.queue().launch(server_name="0.0.0.0", server_port=args.port)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="OSS Chatbot",
        description="Run open source LLMs from HuggingFace with a simple chat interface")

    parser.add_argument("--model-id", default="maywell/Llama-3-Ko-8B-Instruct", help="HuggingFace model name for LLM.")
    parser.add_argument("--port", default=7860, type=int, help="Port number for the Gradio app.")
    parser.add_argument("--use-vllm", action="store_true", help="Use vLLM instead of HuggingFace AutoModelForCausalLM.")
    parser.add_argument("--tensor-parallelism", default=1, type=int, help="Number of tensor parallelism.")
    args = parser.parse_args()

    main(args)
